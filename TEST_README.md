# Test H∆∞·ªõng D·∫´n - Model 3B + Dummy Data

## ‚úÖ Setup Ho√†n T·∫•t

T·∫•t c·∫£ c√°c l·ªói ƒë√£ ƒë∆∞·ª£c s·ª≠a:
1. ‚úÖ Packages installed (transformers 4.57.1, peft 0.18.0, etc.)
2. ‚úÖ Code fixed (imports, cuda calls, etc.)
3. ‚úÖ Dummy dataset created (kh√¥ng c·∫ßn download data)
4. ‚úÖ Test config created (TinyLlama 1.1B)

## üöÄ ƒê·ªÉ Test Code (KH√îNG C·∫¶N DOWNLOAD DATA)

### Option 1: Test Nhanh V·ªõi Model Nh·ªè (~2GB download)

```bash
# Activate environment
conda activate llm

# Ch·∫°y training test v·ªõi dummy data
bash scripts/test_training.sh
```

S·∫Ω:
- Auto-download TinyLlama-1.1B (~2GB) t·ª´ HuggingFace
- T·∫°o 10 ·∫£nh dummy t·ª± ƒë·ªông
- Train 3 steps ƒë·ªÉ verify code ho·∫°t ƒë·ªông
- Kh√¥ng c·∫ßn COCO/RefCOCO dataset

### Option 2: Ch·ªçn Model Kh√°c (3B)

Edit `config/training_configs/test_3b_dummy.py` line 96, uncomment model b·∫°n mu·ªën:

**Phi-3.5-mini (3.8B) - Better quality**
```python
model_name_or_path="microsoft/Phi-3.5-mini-instruct",  # ~8GB download
```

**StableLM-3B**
```python
model_name_or_path="stabilityai/stablelm-3b-4e1t",  # ~6GB download
```

**TinyLlama-1.1B (default) - Fastest**
```python
model_name_or_path="TinyLlama/TinyLlama-1.1B-Chat-v1.0",  # ~2GB download
```

## üìä K·∫øt Qu·∫£ Mong ƒê·ª£i

N·∫øu ch·∫°y th√†nh c√¥ng, b·∫°n s·∫Ω th·∫•y:

```
Step 1/3: loss=X.XXX
Step 2/3: loss=X.XXX
Step 3/3: loss=X.XXX
‚úì Training test completed!
```

## üéØ Training Th·∫≠t V·ªõi Data Th·∫≠t

Sau khi verify code ch·∫°y ƒë∆∞·ª£c, ƒë·ªÉ training v·ªõi data th·∫≠t:

### 1. Download LLaVA checkpoint + RefCOCO data

```bash
bash scripts/download_data.sh
```

S·∫Ω download:
- LLaVA-v1.5-7B (~13GB)
- COCO images (~13-19GB)
- RefCOCO annotations (~500MB)

### 2. Training v·ªõi GPU 4GB

```bash
bash scripts/run_4gb.sh
```

D√πng config: `perception_1gpu_4gb_lora.py` v·ªõi:
- DeepSpeed ZeRO-3 + CPU offloading
- 8-bit quantization
- LoRA rank=8
- Batch size=1

## üìù Files ƒê√£ T·∫°o

```
‚úì config/training_configs/test_3b_dummy.py  - Test config
‚úì mllm/dataset/dummy_dataset.py            - Dummy dataset
‚úì scripts/test_training.sh                 - Test script
‚úì scripts/note.txt                         - Model options
‚úì TEST_README.md                           - This file
```

## üêõ Troubleshooting

### ImportError with peft
```bash
pip install --upgrade 'transformers>=4.45.0'
```

### CUDA Out of Memory
- D√πng TinyLlama thay v√¨ model l·ªõn h∆°n
- Gi·∫£m `image_token_len` t·ª´ 64 xu·ªëng 32
- Gi·∫£m `max_length` t·ª´ 512 xu·ªëng 256

### Model download slow
- D√πng HuggingFace mirror: `HF_ENDPOINT=https://hf-mirror.com`
- Ho·∫∑c download manually v√† update `model_name_or_path`

## ‚ÑπÔ∏è Th√¥ng Tin GPU

- **GPU**: NVIDIA GeForce GTX 1650 with Max-Q Design
- **VRAM**: 3.8 GB
- **CUDA**: 12.1
- **Limitations**:
  - Kh√¥ng support TF32 (Ampere+ only)
  - Training s·∫Ω ch·∫≠m v·ªõi CPU offloading
  - Ch·ªâ ph√π h·ª£p LoRA, kh√¥ng full fine-tune

## ‚ú® Next Steps

1. **Test code**: `bash scripts/test_training.sh` (5-10 ph√∫t)
2. **Download data**: `bash scripts/download_data.sh` (1-2 gi·ªù)
3. **Real training**: `bash scripts/run_4gb.sh` (3-6 gi·ªù)

---

**Prepared by**: Claude Code
**Last Updated**: 2025-11-14
